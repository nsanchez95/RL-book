\documentclass{article}[12pt]
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{times}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\def \R {\mathbb R}
\def \imp {\Longrightarrow}
\def \eps {\varepsilon}
\def \Inf {{\sf Inf}}
\newenvironment{proof}{{\bf Proof.  }}{\hfill$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}{Claim}[section]
\setlength {\parskip}{2pt}
\setlength{\parindent}{0pt}

\newcommand{\headings}[4]{\noindent {\bf Assignment 7 CME241} \hfill {{\bf Author:} Nicolas Sanchez} \\
{} \hfill {{\bf Due Date:} #2} \\

\rule[0.1in]{\textwidth}{0.025in}
}

\newcommand{\klnote}[1]{{\color{red} #1}}
\newcommand{\klsout}[1]{{\color{red} \sout{#1}}}

\begin{document}

\headings{\#1}{Tuesday, October 8, 10:30am}\section{} 



\section{Merton's Continuous Time Formulation - TO FINISH}
We have the following setup:
\begin{align*}
dW_t = ((r + \pi_t(\mu-r))W_t - c_t)dt + \pi_t\sigma W_t dz_t\\
U(x) = \log(x)\\
B(T) = \epsilon
\end{align*}
As our process, utility function and bequest functions. Our goal is the find the optimal $(c_t,\pi_t)$ to maximise the value function:
$$ V(t,W_t) =  E[\int_{s = t}^T e^{-\rho(s-t)}\log(c_t)ds+ e^{-\rho(T-t)}\log(\epsilon\log(W_T)) | W_t]$$
By HJB, we have for the optimal value function $V^*$:

\begin{align*}
\rho V^*(t,W_t) dt &= \max_{(\pi_t, c_t)}[dV^*(t,W_t) + \log(c_t)]\\
&= \max_{(\pi_t, c_t)}[\frac{\partial V}{\partial t} + \frac{\partial V}{\partial W_t}(r + \pi_t(\mu-r))W_t - c_t)+ \frac{\partial^2 V}{\partial W_t^2}\pi_t^2\sigma^2W_t^2/2 +\log(c_t)]\\
&= \max_{(\pi_t, c_t)}\Phi(W_t, t, \pi_t, c_t)\\
\end{align*}

By applying Ito's lemma for the second line. We find first order conditions for $\Phi$:
\begin{align*}
\frac{\partial \Phi}{\partial \pi_t} &= (\mu-r)W_t\frac{\partial V}{\partial W_t} + \pi_rW_t^2\sigma^2 \frac{\partial^2 V}{\partial W_t^2} = 0\\
\frac{\partial \Phi}{\partial c_t} &= -\frac{\partial V}{\partial W_t} + \frac{1}{c}
\end{align*}
Which yield optimal values:
\begin{align*}
\pi_t^*&= \frac{(r-\mu)\frac{\partial V}{\partial W_t}}{W_t\sigma^2\frac{\partial^2 V}{\partial W_t^2}} \\
c_t^* &= \frac{1}{\frac{\partial V}{\partial W_t}}
\end{align*}
Substituting in gives:


\begin{align*}
\rho V^*(t,W_t)  &= \frac{\partial V}{\partial t} + \frac{\partial V}{\partial W_t}(rW_t)-\frac{(\frac{\partial V}{\partial W_t})^2(\mu-r)^2}{2\sigma^2\frac{\partial^2 V}{\partial W_t^2}} - 1-\log(\frac{\partial V}{\partial W_t})
\end{align*}

NEED TO FIND THE RIGHT GUESS FOR FUNCTION. GUESS FROM CLASS DOES NOT WORK :( 

Based on the solution from class we guess a solution of the form:
\begin{align*}
V^*(t,W_t) &= f(t)\log(W_t) \\
\frac{\partial V}{\partial t} = f'(t)\log(W_t)\\
\frac{\partial V}{\partial W_t} = \frac{f(t)}{W_t}\\
\frac{\partial^2 V}{\partial W_t^2} = -\frac{f(t)}{W_t^2}
\end{align*}
Which substituting in yields:
\begin{align*}
f(t)\log(W_t) &=  f'(t)\log(W_t) + rf(t)+\frac{(\mu-r)^2}{2\sigma^2}f(t) - 1-\log(\frac{f(t)}{W_t})\\
\end{align*}
Which reduces to:
\begin{align*}
f'(t) = \log(W_t) &=  f'(t)\log(W_t) + rf(t)+\frac{(\mu-r)^2}{2\sigma^2}f(t) - 1-\log(\frac{f(t)}{W_t})\\
\end{align*}


\section{Speed improvement asset alloc discrete - TO FINISH}

I am hitting somewhat of a fundamental road block for implementing the new expectation. The expectation() in the distribution class must be for any function f (passed as argument) and in my mind that hence requires, in the most general case, something that resembles an integration, notably the evaluation of f on a variety of possible samples. This is exactly what sampledDistribution does... So should our implementation derive an analytical simplification for the specific function f that will be called in ADP. Or should we somehow be doing more efficient sampling?

\section{Career Path MDP}
We denote $s$ as the skill level, $\alpha$ the number of minutes spent working, $p$ the probability of job loss and $\lambda$ the half time of unemployed time. We create the state space as three entries $(s,j,d)$ where $s$ is the skill level, $j$ is a boolean which is indicates if the person is employed on this day. $d$ is the day number.

$$ \mathbf{S} = \{(s,j,d) | s\in \mathbb{R}, j \in \text{"Employed", "Unemployed"}, d\mathbb{N}\} $$
The action would be the time spent working $\alpha$ or the constant action:
$$ \mathbf{A} = \{\alpha | \alpha\in (0,M)\}$$
with $M$ the number of minutes available to work/learn in a day.
The reward will directly being the wages paid if employed or nothing if unemployed:
$$\mathbf{R}((s,j,d)) \begin{cases} f(s)\alpha& \text{if $j =$ "Employed}\\ 0 & \text{ if $j =$ "Unemployed}\end{cases} $$
Finally the transition functions will depend on whether the person is employed or not:
\begin{align*}
Pr((s,\text{"UNEMPLOYED"},d+1),\alpha, (s,\text{"EMPLOYED"},d)) = p\\
Pr((s-(\lambda),\text{"EMPLOYED"},d+1),\alpha, (s,\text{"EMPLOYED"},d)) = 1-p\\
Pr((s(1+(\frac{1}{2})^{\lambda^{-1}} g(s),\text{"UNEMPLOYED"},d+1),\alpha, (s,\text{"UNEMPLOYED"},d)) = 1-h(s)\\
Pr((s(1+(\frac{1}{2})^{\lambda^{-1}} g(s),\text{"EMPLOYED"},d+1),\alpha, (s, \text{"UNEMPLOYED"},d)) = h(s)\\
\end{align*}
 
 
 We note that the existence of the $d$ feature means that we could incorporate finite horizon dynamics into the transitions probabilities easily. Different skills could be accommodated using a vector representation of the skill level, in other words $s \in \mathbf{R}^{m}$. This could in turn lead to the action becoming an allocation of time over all the learning skills, ie with $\alpha \in \mathbf{N}_+^{m+1}$ (all skills + working) note that we restrict this action space to $\sum_i \alpha_i = M$. Multiple job options would likely require to convert the single probability $h(s)$ into a multivariate function which maps the vector $s$ to a vector of probabilities with all entries between 0 and 1 corresponding to the probability of getting a job for each potential job. 
\end{document}
