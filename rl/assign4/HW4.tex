\documentclass{article}[12pt]
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{times}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\def \R {\mathbb R}
\def \imp {\Longrightarrow}
\def \eps {\varepsilon}
\def \Inf {{\sf Inf}}
\newenvironment{proof}{{\bf Proof.  }}{\hfill$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}{Claim}[section]
\setlength {\parskip}{2pt}
\setlength{\parindent}{0pt}

\newcommand{\headings}[4]{\noindent {\bf Assignment 2 CME241} \hfill {{\bf Author:} Nicolas Sanchez} \\
{} \hfill {{\bf Due Date:} #2} \\

\rule[0.1in]{\textwidth}{0.025in}
}

\newcommand{\klnote}[1]{{\color{red} #1}}
\newcommand{\klsout}[1]{{\color{red} \sout{#1}}}

\begin{document}

\headings{\#1}{Tuesday, October 8, 10:30am}\section{} 



\section{Manual Value Iteration}
We do the steps of value iteration.
\begin{align*}
v_1(s_1) &= \max\{q_1(s_1,a_1), q_1(s_1,a_2)\} = \max\{ 8 + (0.2*10 + 0.6*1.0), 10+(0.1*10+0.2*1)\} = \max\{10.6, 11.2\} = 11.\\
v_1(s_2) &= \max\{q_1(s_1,a_1), q_1(s_1,a_2)\} =   \max\{ 1+ (0.3*10 + 0.3*1.0),-1+(0.5*10+0.3*1)\} = \max\{4.3, 4.3\} = 4.3\\
\pi_1(s_1) &= a_2\\
\pi_1(s_2) &= a_1\\
v_1(s_1)  &= \max\{ 8 + (0.2*11.2 + 0.6*4.3), 10+(0.1*11.2+0.2*4.3)\} = \max\{12.82, 11.98\} = 12.82\\
v_1(s_2) &= \max\{ 1+ (0.3*11.2 + 0.3*4.3), -1+(0.5*11.2+0.3*4.3)\} = \max\{5.65, 5.89\} = 5.89\\
\pi_2(s_1) &= a_1\\
\pi_2(s_2) &= a_2\\
\end{align*}
We now notice that by construction, that the difference between action $a_1$ and $a_2$ for state $s_1$ grows if the values grow, or specifically $q_k(s_1,a_1) - q_k(s_1,a_2) > q_{k-1}(s_1,a_1) - q_{k-1}(s_1,a_2)$ if $v_k(s) > v_{k-1}(s) $ because of the strictly higher likelihoods to go to each of these states. Since only the transition probs are all positive, the values of the states will monotonically increase, meaning we will always have $q_k(s_1,a_1) > q_k(s_1,a_2)$. An analogous argument works for  $q_k(s_2,a_2) > q_k(s_2,a_1)$ and leaves us with the optimal policy:
\begin{align*}
\pi(s_1) &= a_1\\
\pi(s_2) &= a_2\\
\end{align*}
\section{Frog Croaking Revisited}

Both policy and value iteration are orders and orders of magnitude faster than the exhaustive grid search. Indeed for 25 lilypads, value iteration takes 120 iteration and policy iteration takes 4 vs the 33mm possible policies in the exhaustive search. Interestingly the limited number of actions as well as the symmetric aspect of the problem for anything not at the beginning of the track means that policy iteration actually does not take longer to converge as the problem grows bigger.

\begin{figure}
  \includegraphics[width=\linewidth]{CovergenceSpeed.png}
  \caption{Iterations needed for convergence}
  \label{fig:llp3}
\end{figure}

\section{Job Hopping and Wages-Utility Maximisation}
The state will be a tuple $s_t = (s,i)$ with $s\in\{\tilde{e}, \tilde{u}\}$ and $i \in [1,2,\ldots,n]$.\\
The action space will be:
\begin{align*}
A_t(s,i) = \begin{cases} \{A\} \text{ if $s = \tilde{e}$} \\ \{A, R\} \text{ if $s=\tilde{u}$}\end{cases}
\end{align*}
The transition probabilities will be as follows:
\begin{align*}
P((s,i), A, (\tilde{e},j) &= (1-\alpha)\delta_{ij}\\
P((s,i), A, (\tilde{u},j) &= \frac{\alpha}{n}\\
P((\tilde{u},i), R, (\tilde{u},j) &= \frac{1}{n}\\
\end{align*}
With all other transition probabilities being zero. The reward function is:
\begin{align*}
R((\tilde{e},i), A, (s,j)) = \log(w_i)\\
R((\tilde{u},i), A, (\tilde{e},j) = \log(w_j)\\
R((\tilde{u},i), R, (s,j) = \log(w_0)\\
\end{align*}

This yields the following Bellman equations:
\begin{align*}
V(\tilde{e}, i) &= \log(w_i) + (1-\alpha)\gamma V(\tilde{e},i) + \frac{\alpha\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)\\
V(\tilde{u}, i) &= \max\{\log(w_i) + (1-\alpha)\gamma V(\tilde{e},i) + \frac{\gamma\alpha}{n} \sum_{j=1}^n V(\tilde{u}, j), \log(w_0)+ \frac{\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)\}\\
\end{align*}
And we can rearrange these to get:
\begin{align*}
V(\tilde{e}, i) &= \frac{\log(w_i) + \frac{\alpha\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)}{1-\gamma(1-\alpha)}\\
V(\tilde{u}, i)& =  \max\{V(\tilde{e},i), \log(w_0)+ \frac{\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)\} \\
\end{align*}
Which one step further yields:
\begin{align*}
V(\tilde{e}, i) &= \frac{\log(w_i) + \frac{\alpha\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)}{1-\gamma(1-\alpha)}\\
V(\tilde{u}, i)& =  \max\{ \frac{\log(w_i) + \frac{\alpha\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)}{1-\gamma(1-\alpha)}, \log(w_0)+ \frac{\gamma}{n} \sum_{j=1}^n V(\tilde{u}, j)\} \\
\end{align*}

We can now hence implement an algorithm that repeatedly updates $V(\tilde{u}, i)$ and then once it converges, calculate the final $V(\tilde{e}, i)$. This is implemented in worker.py. The start point for $V(\tilde{u}, i)$ is the "worst case" scenario of being unemployed for ever: $\frac{1}{1-\gamma}{\log(w_0)}$.


\end{document}
